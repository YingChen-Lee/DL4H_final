{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50615,"status":"ok","timestamp":1652054513691,"user":{"displayName":"Ying-Chen Lee","userId":"16343670133736057324"},"user_tz":-480},"id":"D-uf6ZwpENIy","outputId":"33252f64-7af0-4102-a463-543cd93e97e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/.shortcut-targets-by-id/14VE6jxu6gzBuYO9WY79TIloM87C-NZXP/598-DLH/Draft\n","'Check features in chartevents.ipynb'        store_data_in_tensor_5-7-22.ipynb\n"," CS598DL4H_project_template_2022Spring.pdf   store_data_in_tensor_5-8-22.ipynb\n"," DL4H_team_149_Draft.pdf                    'store data in tensor.ipynb'\n","'old code.gdoc'                             'store notes in tensor.ipynb'\n","'Preprocessing Notebook.ipynb'               \u001b[0m\u001b[01;34mtensor_exports\u001b[0m/\n"," \u001b[01;34mpreprocess_text\u001b[0m/                            training.ipynb\n","'Project Draft instructions.gdoc'           'Useful links.gdoc'\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/598-DLH/Draft\n","%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xjhXSwM0ESGi"},"outputs":[],"source":["import torch\n","import pandas as pd\n","from datetime import datetime\n","from datetime import timedelta\n","from math import ceil\n","import gzip"]},{"cell_type":"code","source":["!pip3 install accelerate\n","from accelerate import Accelerator"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-077QuNeGdtR","executionInfo":{"status":"ok","timestamp":1652054518115,"user_tz":-480,"elapsed":2940,"user":{"displayName":"Ying-Chen Lee","userId":"16343670133736057324"}},"outputId":"e281e655-8d18-48da-f33d-badfa24dc802"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: accelerate in /usr/local/lib/python3.7/dist-packages (0.7.1)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.11.0+cu113)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from accelerate) (3.13)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (4.2.0)\n"]}]},{"cell_type":"code","source":["%cd tensor_exports"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zycz2OXvfbPn","executionInfo":{"status":"ok","timestamp":1652054518118,"user_tz":-480,"elapsed":23,"user":{"displayName":"Ying-Chen Lee","userId":"16343670133736057324"}},"outputId":"14da466c-9099-4ba9-f6a2-51af6fea4137"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/14VE6jxu6gzBuYO9WY79TIloM87C-NZXP/598-DLH/Draft/tensor_exports\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9v44rDBvEl4x"},"outputs":[],"source":["final_tensor = torch.load('data_tensor_final.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59,"status":"ok","timestamp":1652054613625,"user":{"displayName":"Ying-Chen Lee","userId":"16343670133736057324"},"user_tz":-480},"id":"nXP2cF2HEu7q","outputId":"73177176-bfff-4b88-ef2b-7e2b466c4572"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1541, 5, 720, 826])\n"]}],"source":["print(final_tensor.shape)"]},{"cell_type":"code","source":["FEATURE_SIZE = int(final_tensor.shape[3])\n","BATCH_SIZE=100\n","LEARNING_RATE=1e-2"],"metadata":{"id":"gLWp_CxlaqMn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HSDSnS7eRCf_"},"source":["# Try to get the hadm_idx_to_hadm_id first\n","so that we can get the is_sepsis label for that hadm_id"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":47,"status":"ok","timestamp":1652054613629,"user":{"displayName":"Ying-Chen Lee","userId":"16343670133736057324"},"user_tz":-480},"id":"EX3_b8vmr_5B","outputId":"8eeee6a3-63e3-4b64-f6b1-2345f87bf2e4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n%cd ../mimic\\n%ls\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}],"source":["'''\n","%cd ../mimic\n","%ls\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IdgMI_MNsKgL"},"outputs":[],"source":["final_icustays_details = pd.read_pickle('final_icustays_details.pkl')\n","def get_idx_to_hadm_index_dict(values_pdSeries):\n","    return {i:v for i, v in enumerate(list(values_pdSeries.unique()))}\n","    \n","hadm_idx_id_dict = get_idx_to_hadm_index_dict(final_icustays_details['hadm_id'])\n","hadm_sepsis = pd.read_pickle('hadm_sepsis.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8uApVIt4vjAU"},"outputs":[],"source":["hadm_id_isSepsis_dict = dict(zip(hadm_sepsis.hadm_id, hadm_sepsis.is_sepsis))"]},{"cell_type":"markdown","metadata":{"id":"YMB33ywaYOSD"},"source":["# DataLoadder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KJ6q-oT7YPd3"},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, data_tensor, hadm_id_isSepsis_dict, hadm_idx_id_dict):\n","        self.data_tensor = data_tensor\n","        self.hadm_id_isSepsis_dict = hadm_id_isSepsis_dict\n","        self.hadm_idx_id_dict = hadm_idx_id_dict\n","        self._get_available_icu_stays()\n","        return\n","\n","    def __len__(self):\n","        return len(self.available_icu_stays)\n","\n","    def _get_available_icu_stays(self): ## ICU stays that are not padded\n","        self.available_icu_stays = []\n","        for hadm_idx in range(self.data_tensor.shape[0]):\n","            for icu_idx in range(self.data_tensor.shape[1]):\n","                if self.data_tensor[hadm_idx, icu_idx, 0, 2] != 0: ## If the first ICU stay's length of stay (los) is zero, then it's padded\n","                    self.available_icu_stays.append((hadm_idx, icu_idx))\n","\n","    def _get_sofa_score(self, index, value):\n","        if index == 57: ## Platelets\n","            if value >= 150:\n","                return 0\n","            elif value >= 100:\n","                return 1\n","            else:\n","                return 2\n","        elif index == 34: ## creatinie  # => seems to over estimate\n","            if value < 1.2:\n","                return 0\n","            elif value < 2:\n","                return 1\n","            else:\n","                return 2\n","        elif index == 20: ## FiO2\n","            return 2 if value >=53.3 else 0\n","        else:\n","            return 0\n","        \n","       \n","\n","    def _get_sepsis_onset_hour_or_last_hour(self, x):\n","        hour_idx = 0\n","        for hour in x:\n","            SOFA_score = self._get_sofa_score(57, hour[57]) + self._get_sofa_score(20, hour[20]) + self._get_sofa_score(34, hour[34])\n","            if SOFA_score >= 2:\n","                return hour_idx, torch.tensor(1, dtype=torch.float32)\n","            hour_idx += 1\n","        return int(x[0][2]*24), torch.tensor(0, dtype=torch.float32) ## Get the first hour's 3rd feature, which is length of stay of this icu\n","  \n","    def __getitem__(self, index):\n","        target_hadm_idx, target_icu_idx = self.available_icu_stays[index]\n","        x = self.data_tensor[target_hadm_idx, target_icu_idx] \n","\n","        l = int(x[0][2]*24) ## Get the first hour's 3rd feature, which is length of stay of this icu\n","        target_hadm_id = self.hadm_idx_id_dict[target_hadm_idx]\n","        if self.hadm_id_isSepsis_dict[target_hadm_id] == False:\n","            y = torch.tensor(0, dtype=torch.float32)\n","        else:\n","            l, y = self._get_sepsis_onset_hour_or_last_hour(x)\n","        return x, y, l\n","\n","dataset = CustomDataset(final_tensor, hadm_id_isSepsis_dict, hadm_idx_id_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1652054614438,"user":{"displayName":"Ying-Chen Lee","userId":"16343670133736057324"},"user_tz":-480},"id":"edi4AX1HxQYj","outputId":"930d6270-f4ef-4e85-f10e-b021c6b9b496"},"outputs":[{"output_type":"stream","name":"stdout","text":["length of train dataset: 1434\n","length of test dataset: 359\n"]}],"source":["from torch.utils.data.dataset import random_split\n","\n","split = int(len(dataset) * 0.8)\n","lengths = [split, len(dataset) - split]\n","train_dataset, test_dataset = random_split(dataset, lengths)\n","print(\"length of train dataset:\", len(train_dataset))\n","print(\"length of test dataset:\", len(test_dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A1QkPsVq0eyP"},"outputs":[],"source":["def collate_fn(data):\n","    sequences, labels, lengths = zip(*data)\n","    x = torch.stack(sequences, dim=0)\n","    y = torch.tensor(labels, dtype=torch.float)  \n","    l = torch.tensor(lengths, dtype=torch.long)\n","    return x, y, l\n","  \n","def get_last_visit(hidden_states, length):\n","    return hidden_states[range(hidden_states.shape[0]), length - 1, :]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":610,"status":"ok","timestamp":1652054615038,"user":{"displayName":"Ying-Chen Lee","userId":"16343670133736057324"},"user_tz":-480},"id":"1LQblsJW6OV2","outputId":"1908e0e0-9227-43e3-8711-a1331d14037c"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 720, 826])\n","torch.Size([2])\n","tensor([ 0, 44])\n"]}],"source":["\n","## test\n","from torch.utils.data import DataLoader\n","loader = DataLoader(train_dataset, batch_size=2, collate_fn=collate_fn)\n","loader_iter = iter(loader)\n","x, y, l = next(loader_iter)\n","\n","print(x.shape)\n","print(y.shape)\n","print(l)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PoH236Tm6V8l"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hbvl10-4_JTn"},"outputs":[],"source":["def get_last_visit(hidden_states, length):\n","    return hidden_states[range(hidden_states.shape[0]), length - 1, :]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1652054615042,"user":{"displayName":"Ying-Chen Lee","userId":"16343670133736057324"},"user_tz":-480},"id":"jVhv9jOV_7vt","outputId":"2837d480-cc82-4735-a5a3-3a8d81fa3878"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n## TEST \\nimport random\\nmax_num_visits = 10\\nbatch_size = 16\\nhidden_dim = 100\\n\\nhidden_states = torch.randn((batch_size, max_num_visits, hidden_dim))\\nlengths = torch.tensor([random.randint(1, max_num_visits) for _ in range(batch_size)])\\nout = get_last_visit(hidden_states, lengths)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}],"source":["'''\n","## TEST \n","import random\n","max_num_visits = 10\n","batch_size = 16\n","hidden_dim = 100\n","\n","hidden_states = torch.randn((batch_size, max_num_visits, hidden_dim))\n","lengths = torch.tensor([random.randint(1, max_num_visits) for _ in range(batch_size)])\n","out = get_last_visit(hidden_states, lengths)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":287,"status":"ok","timestamp":1652054615312,"user":{"displayName":"Ying-Chen Lee","userId":"16343670133736057324"},"user_tz":-480},"id":"zXklYM_5-_Cz","outputId":"c43c6765-d04c-4e47-f03b-87d3fa8e67d1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["RNN(\n","  (rnn): LSTM(826, 800, batch_first=True)\n","  (fc): Linear(in_features=800, out_features=2, bias=True)\n","  (softmax): Softmax(dim=1)\n",")"]},"metadata":{},"execution_count":18}],"source":["\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","class RNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.rnn = nn.LSTM(input_size = FEATURE_SIZE, hidden_size = 800, batch_first=True,dtype=torch.float16)\n","        self.fc = nn.Linear(800, 2)\n","        self.softmax = nn.Softmax(dim=1)\n","  \n","    def forward(self, x, length):\n","        batch_size = x.shape[0]\n","        x = x.to(torch.float16)\n","        output, _ = self.rnn(x)\n","        true_h_n = get_last_visit(output, length)\n","        logits = torch.relu(self.fc(true_h_n))\n","        probs = self.softmax(logits)\n","        '''\n","        pred = []\n","        for res in probs:\n","          pred.append(res)\n","        '''\n","        return probs\n","\n","model = RNN()\n","model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jf3_FFy0FRwd"},"outputs":[],"source":["criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"]},{"cell_type":"code","source":["### TPU\n","accelerator = Accelerator()\n","device = accelerator.device\n","model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)"],"metadata":{"id":"buDrfMRCGxiG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"txbEHx83F0qj"},"outputs":[],"source":["from sklearn.metrics import *\n","\n","#input: Y_score,Y_pred,Y_true\n","#output: accuracy, auc, precision, recall, f1-score\n","def classification_metrics(Y_score, Y_pred, Y_true):\n","    auc, precision, recall =   roc_auc_score(Y_true, Y_score), \\\n","                               precision_score(Y_true, Y_pred), \\\n","                                recall_score(Y_true, Y_pred)\n","    return auc, precision, recall\n","\n","\n","#input: model, loader\n","def evaluate(model, loader):\n","    model.eval()\n","    all_y_true = torch.LongTensor()\n","    all_y_pred = torch.LongTensor()\n","    all_y_score = torch.FloatTensor()\n","    \n","\n","    for x, y, l in loader:\n","\n","        # pass the input through the model\n","        y_hat = model(x, l)\n","        y_hat = y_hat.select(dim=1, index=0)\n","        # convert shape from [batch size, 1] to [batch size]\n","        y_pred = (y_hat > 0.5).type(torch.float16)\n","        \n","        y_hat = torch.nan_to_num(y_hat)\n","\n","        all_y_true = torch.cat((all_y_true, y.to('cpu')), dim=0)\n","        all_y_pred = torch.cat((all_y_pred,  y_pred.to('cpu')), dim=0)\n","        all_y_score = torch.cat((all_y_score,  y_hat.to('cpu')), dim=0)\n","    \n","    all_y_true = torch.cat((torch.tensor([1]), all_y_true.to('cpu')), dim=0) ## According to the paper, it up-scaling the septic cases\n","    all_y_pred = torch.cat((torch.tensor([1]), all_y_pred.to('cpu')), dim=0)\n","    all_y_score = torch.cat((torch.tensor([1]), all_y_score.to('cpu')), dim=0)\n","\n","    acc, auc, precision, recall, f1 = classification_metrics(all_y_score.detach().numpy(), \n","                                                             all_y_pred.detach().numpy(), \n","                                                             all_y_true.detach().numpy())\n","    print(f\"auc: {auc:.3f}, precision: {precision:.3f}, recall: {recall:.3f}\")\n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L_h8mUiVIsR0"},"outputs":[],"source":["#train_init = evaluate(model, train_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I69GcjfWF-Sc"},"outputs":[],"source":["n_epochs = 50\n","\n","# prep model for training\n","for epoch in range(n_epochs):\n","    model.train()\n","    train_loss = 0\n","    for x, y, l in train_loader:\n","        x = torch.nan_to_num(x)\n","        \"\"\" Step 1. clear gradients \"\"\"\n","        optimizer.zero_grad()\n","        \"\"\"  Step 2. perform forward pass using `model`, save the output to y_hat \"\"\"\n","        y_hat = model(x, l)\n","        \"\"\" Step 3. calculate the loss using `criterion`, save the output to loss. \"\"\"\n","        \n","        #y_hat = torch.nan_to_num(y_hat)\n","        y_hat = torch.select(y_hat, index=0, dim=1)\n","        \n","        loss = criterion(y_hat, y)\n","        \"\"\" Step 4. backward pass \"\"\"\n","        #loss.backward()\n","        accelerator.backward(loss)\n","        \"\"\" Step 5. optimization \"\"\"\n","        optimizer.step()\n","        \"\"\" Step 6. record loss \"\"\"\n","        train_loss += loss.item()\n","        \n","    train_loss = train_loss / len(train_loader)\n","    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n","    evaluate(model, train_loader)\n","    evaluate(model, test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yx3fLKQvJzp1"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","name":"training.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":0}