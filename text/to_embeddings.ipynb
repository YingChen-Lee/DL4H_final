{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae3092c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74e84f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef remove_date(text):\\n    return re.sub(r'\\\\d{4}\\\\-\\\\d{1,2}\\\\-\\\\d{1,2}', ' ', text)\\n\\ndef remove_special_characters(text):\\n    text = re.sub(r'[^a-zA-Z0-9-]', ' ', text)\\n    ## Remove '-' but keep negative  (i.e. 'sign-in' => 'sign in', '------', => '' , '-123' => '-123') \\n    text = re.sub(r'(\\\\s+)(\\\\-)(\\\\d+)', r'\\x01#\\x03', text)\\n    text = re.sub(r'\\\\-',' ', text)\\n    text = re.sub(r'#', '-', text)\\n    return text\\n\\ndef remove_multiple_spaces(text):\\n    return re.sub('\\\\s+', ' ', text).strip()\\n\\ndef process_text_into_words(text):\\n    text = remove_date(text)\\n    text = remove_special_characters(text)\\n    text = remove_multiple_spaces(text.lower())\\n    text = remove_stopwords(text)\\n    words = word_tokenize(text)\\n    return words\\n\\ndef process_text_into_word_lists(text):\\n    sentences = text.split('\\n')\\n    output_sentence = list(map(process_text_into_words, sentences))\\n    output_sentence = list(filter(None, output_sentence))\\n    return output_sentence\\n\\n\\nREAD_PROPORTION_FOR_NOTES = 30\\nnotes = pd.read_csv('NOTEEVENTS.csv.gz',skiprows = [i for i in range(1, 2083180*(READ_PROPORTION_FOR_NOTES-1) // READ_PROPORTION_FOR_NOTES) ], compression='gzip') #only 1619465 has charttime\\nnotes.columns = notes.columns.str.lower()\\n\\nimport itertools\\nnotes['text'] = notes['text'].map(process_text_into_word_lists)\\nnotes['text_bert'] = notes['text'].map(lambda sentences: [' '.join(sentence) for sentence in sentences])\\nnotes['text_tfidf']= notes['text'].map(lambda sentences: list(itertools.chain(*sentences)))\\nnotes = notes.drop(columns=['text']).reset_index(drop=True)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def remove_date(text):\n",
    "    return re.sub(r'\\d{4}\\-\\d{1,2}\\-\\d{1,2}', ' ', text)\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9-]', ' ', text)\n",
    "    ## Remove '-' but keep negative  (i.e. 'sign-in' => 'sign in', '------', => '' , '-123' => '-123') \n",
    "    text = re.sub(r'(\\s+)(\\-)(\\d+)', r'\\1#\\3', text)\n",
    "    text = re.sub(r'\\-',' ', text)\n",
    "    text = re.sub(r'#', '-', text)\n",
    "    return text\n",
    "\n",
    "def remove_multiple_spaces(text):\n",
    "    return re.sub('\\s+', ' ', text).strip()\n",
    "\n",
    "def process_text_into_words(text):\n",
    "    text = remove_date(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_multiple_spaces(text.lower())\n",
    "    text = remove_stopwords(text)\n",
    "    words = word_tokenize(text)\n",
    "    return words\n",
    "\n",
    "def process_text_into_word_lists(text):\n",
    "    sentences = text.split('\\n')\n",
    "    output_sentence = list(map(process_text_into_words, sentences))\n",
    "    output_sentence = list(filter(None, output_sentence))\n",
    "    return output_sentence\n",
    "\n",
    "\n",
    "READ_PROPORTION_FOR_NOTES = 30\n",
    "notes = pd.read_csv('NOTEEVENTS.csv.gz',skiprows = [i for i in range(1, 2083180*(READ_PROPORTION_FOR_NOTES-1) // READ_PROPORTION_FOR_NOTES) ], compression='gzip') #only 1619465 has charttime\n",
    "notes.columns = notes.columns.str.lower()\n",
    "\n",
    "import itertools\n",
    "notes['text'] = notes['text'].map(process_text_into_word_lists)\n",
    "notes['text_bert'] = notes['text'].map(lambda sentences: [' '.join(sentence) for sentence in sentences])\n",
    "notes['text_tfidf']= notes['text'].map(lambda sentences: list(itertools.chain(*sentences)))\n",
    "notes = notes.drop(columns=['text']).reset_index(drop=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93d0959b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013740\n"
     ]
    }
   ],
   "source": [
    "READ_PROPORTION_FOR_NOTES = 30\n",
    "print(2083180*(READ_PROPORTION_FOR_NOTES-1) // READ_PROPORTION_FOR_NOTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e71df15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_pickle('cleaned_text.pkl')\n",
    "notes = notes[notes['text_bert'].map(lambda d: len(d)) > 0]\n",
    "notes = notes[notes['text_tfidf'].map(lambda d: len(d)) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "178972d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install transformers # => need to pip install at the first time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "config.update({'output_hidden_states':True})  ## Because we only want the last 4 hidden layers\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", config=config)\n",
    "old_model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a70bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We use only last 4 layers in the BERT model as mentioned in the paper\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "def deleteEncodingLayers(model):  # must pass in the full bert model\n",
    "    oldModuleList = model.encoder.layer\n",
    "    newModuleList = nn.ModuleList()\n",
    "    # Now iterate over all layers, only keepign only the relevant layers.\n",
    "    for i in range(8, 12):\n",
    "        newModuleList.append(oldModuleList[i])\n",
    "\n",
    "    # create a copy of the model, modify it with the new list, and return\n",
    "    copyOfModel = copy.deepcopy(model)\n",
    "    copyOfModel.encoder.layer = newModuleList\n",
    "\n",
    "    return copyOfModel\n",
    "\n",
    "model = deleteEncodingLayers(old_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "686f605a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n## Mean count of tokens for each sentences is 10\\ndef count_mean(sentence):\\n    tmp = [len(x.split()) for x in sentence]\\n    return (sum(tmp)/ len(tmp))\\n\\nprint((notes['text_bert'].map(count_mean)).mean())\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## Mean count of tokens for each sentences is 10\n",
    "def count_mean(sentence):\n",
    "    tmp = [len(x.split()) for x in sentence]\n",
    "    return (sum(tmp)/ len(tmp))\n",
    "\n",
    "print((notes['text_bert'].map(count_mean)).mean())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e25411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_wise_embedding(sentences):\n",
    "\n",
    "    features = tokenizer(\n",
    "        sentences,\n",
    "        padding='max_length',\n",
    "        max_length=10, ## use mean count of tokens\n",
    "        truncation=True,\n",
    "        return_tensors='pt', ## 'pt': Return PyTorch torch.Tensor objects\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    mask = features['attention_mask']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**features)\n",
    "\n",
    "    #all_hidden_states = outputs['last_hidden_state']\n",
    "\n",
    "    #concatenate_pooling = torch.cat(\n",
    "        #(all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]), 0)\n",
    "    #concatenate_mask = torch.cat(\n",
    "        #(mask,mask,mask,mask), 0)\n",
    "\n",
    "    concatenate_pooling = outputs['last_hidden_state']\n",
    "    #concatenate_mask = mask\n",
    "        \n",
    "    #concatenate_mask = concatenate_mask.unsqueeze(-1)\n",
    "    #mask_embeddings = concatenate_pooling * concatenate_mask.float()\n",
    "    mask_embeddings = concatenate_pooling\n",
    "    \n",
    "    #mean_mask_embeddings = mask_embeddings.sum(dim=1)\n",
    "    #for dim in range(concatenate_mask.size(0)):\n",
    "    #    mean_mask_embeddings[dim] = mean_mask_embeddings[dim] / concatenate_mask[dim].sum()\n",
    "    \n",
    "    mean_mask_embeddings = mask_embeddings.mean(dim=1)\n",
    "    document_wise_embedding = mean_mask_embeddings.mean(dim=0)\n",
    "    \n",
    "    return document_wise_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d827c0",
   "metadata": {},
   "source": [
    "REFERENCE:\n",
    "https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently/notebook\n",
    "https://stackoverflow.com/questions/71434804/how-to-fed-last-4-concatenated-hidden-layers-of-bert-to-fc-layers\n",
    "https://towardsdatascience.com/how-to-use-bert-from-the-hugging-face-transformer-library-d373a22b0209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b51f6309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6h 3s, sys: 2min 5s, total: 6h 2min 8s\n",
      "Wall time: 1h 30min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "notes['text_bert'] = notes['text_bert'].map(get_document_wise_embedding)\n",
    "notes.to_pickle('text_bert.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce4fe3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
